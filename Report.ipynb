{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report \n",
    "\n",
    "## Summary \n",
    "\n",
    "For this project,the Deep Deterministic Policy Gradient (DDPG) algorithm was used to train the agent.\n",
    "\n",
    "Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a Q-function and a policy.\n",
    "It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy.\n",
    "\n",
    "This approach is closely connected to Q-learning, and is motivated the same way.\n",
    "\n",
    "Some characteristics of DDPG:\n",
    "* DDPG is an off-policy algorithm.\n",
    "* DDPG can only be used for environments with continuous action spaces.\n",
    "* DDPG can be thought of as being deep Q-learning for continuous action spaces.\n",
    "\n",
    "## Model architecture and hyperparameters\n",
    "\n",
    "The model architectures for the two neural networks used for the Actor and Critic are as follows:\n",
    "\n",
    "Actor:\n",
    "* Fully connected layer 1: Input 33 (state space), Output 400, Batch Normalization,and then RELU activation\n",
    "* Fully connected layer 2: Input 400, Output 300, RELU activation\n",
    "* Fully connected layer 3: Input 300, Output 4 (action space), TANH activation\n",
    "\n",
    "Critic:\n",
    "* Fully connected layer 1: Input 33 (state space), Output 400, Batch Normalization, RELU activation\n",
    "* Fully connected layer 2: Input join( 400 + action 4), Output 300, RELU activation\n",
    "* Fully connected layer 3: Input 300, Output 1\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0      # L2 weight decay\n",
    "```\n",
    "\n",
    "## Plot of rewards\n",
    "\n",
    "Below is a training run of the above model archicture and hyperparameters:\n",
    "\n",
    "* Number of agents: 1\n",
    "* Size of each action: 4\n",
    "\n",
    "\n",
    "### 5 Training \n",
    "- **Second submission**\n",
    "\n",
    "After following the project reviewer suggesion,Update the OU_noise function,using the standard random funtion.\n",
    "The agent gets better scores and trained faster than before.\n",
    "\n",
    "![attemp1](./a1.png)\n",
    "\n",
    "\n",
    "\n",
    "* Environment solved in about 120 episodes; \n",
    "* 150 episode, it reaches around 38 average score.\n",
    "\n",
    "## Future work\n",
    "\n",
    "* Change hyper-parameters,to accerate the training speed.\n",
    "* Apply kinematics to limit the space for arm movements and reduce useless movements\n",
    "* Training of multiple agents to perform the activity in parallel\n",
    "* Explore distributed training using\n",
    "\t* A3C - Asynchronous Advantage Actor-Critic\n",
    "\t* PPO - Proximal Policy Optimization Algorithms\n",
    "    * D4PG - DISTRIBUTED DISTRIBUTIONAL DETERMINISTIC POLICY GRADIENTS\n",
    "\n",
    "# Troubleshooting Tips\n",
    "Because udacity workspace will be disconnect from my local machine,if there is not inter-activate with browser,So I using the function torch.save() to save the model weights, it can be re-use again and save so many training time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
