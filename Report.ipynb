{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report \n",
    "\n",
    "## Summary \n",
    "\n",
    "For this project,the Deep Deterministic Policy Gradient (DDPG) algorithm was used to train the agent.\n",
    "\n",
    "Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a Q-function and a policy.\n",
    "It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy.\n",
    "\n",
    "This approach is closely connected to Q-learning, and is motivated the same way.\n",
    "\n",
    "Some characteristics of DDPG:\n",
    "* DDPG is an off-policy algorithm.\n",
    "* DDPG can only be used for environments with continuous action spaces.\n",
    "* DDPG can be thought of as being deep Q-learning for continuous action spaces.\n",
    "\n",
    "## Model architecture and hyperparameters\n",
    "\n",
    "The model architectures for the two neural networks used for the Actor and Critic are as follows:\n",
    "\n",
    "Actor:\n",
    "* Fully connected layer 1: Input 33 (state space), Output 400, Batch Normalization,and then RELU activation\n",
    "* Fully connected layer 2: Input 400, Output 300, RELU activation\n",
    "* Fully connected layer 3: Input 300, Output 4 (action space), TANH activation\n",
    "\n",
    "Critic:\n",
    "* Fully connected layer 1: Input 33 (state space), Output 400, Batch Normalization, RELU activation\n",
    "* Fully connected layer 2: Input join( 400 + action 4), Output 300, RELU activation\n",
    "* Fully connected layer 3: Input 300, Output 1\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0      # L2 weight decay\n",
    "EPSILON = 1.0           # explore->exploit noise process added to act step\n",
    "EPSILON_DECAY = 1e-6    # decay rate for noise process\n",
    "```\n",
    "\n",
    "## Plot of rewards\n",
    "\n",
    "Below is a training run of the above model archicture and hyperparameters:\n",
    "\n",
    "* Number of agents: 1\n",
    "* Size of each action: 4\n",
    "\n",
    "\n",
    "### 5 attempt training \n",
    "- **Attempt 0**\n",
    "Using cpu to train the agent 200 episodes,it takes about 68 minutes to train 200 episodes.\n",
    "![attemp1](./a1.png)\n",
    "- **Attempt 1**\n",
    "Continue to train from last time and setting episode to 600,workspace was broken at episode 345 and got average score:24.63\n",
    "![attemp2](./a2.png)\n",
    "- **Attempt 2**\n",
    "The workspace was broken again,continue to train from last time(load back the model weights) and setting episode to 500,but the workspace can't last to the 500 episode,it was broken again.Luckly, it shows the average score reaches to 33.04 in 342 episode.\n",
    "![attemp3](./a3.png)\n",
    "- **Attempt 3**\n",
    "It takes too long time to train if using cpu, so i enable the GPU this time.first set 500 episode train in GPU. it takes about 72 minutes to train 500 episode and max time step is 1000. it got 16.19 score in 500 episode.\n",
    "![attemp4](./a4.png)\n",
    "- **Attempt 4**\n",
    "continue to train in GPU from last time(load back the model weights),Environment solved in about 700~800 episode.\n",
    "![attemp5](./a5.png)\n",
    "\n",
    "\n",
    "* Environment solved in about 800 episodes,\tAverage Score: 32.12; in 1000 episode, it reaches 33.31 average score.\n",
    "\n",
    "## Future work\n",
    "\n",
    "* Change hyper-parameters,to accerate the training speed.\n",
    "* Apply kinematics to limit the space for arm movements and reduce useless movements\n",
    "* Training of multiple agents to perform the activity in parallel\n",
    "* Explore distributed training using\n",
    "\t* A3C - Asynchronous Advantage Actor-Critic\n",
    "\t* PPO - Proximal Policy Optimization Algorithms\n",
    "    * D4PG - DISTRIBUTED DISTRIBUTIONAL DETERMINISTIC POLICY GRADIENTS\n",
    "\n",
    "# Troubleshooting Tips\n",
    "Because udacity workspace will be disconnect from my local machine,if there is not inter-activate with browser,So I using the function torch.save() to save the model weights, it can be re-use again and save so many training time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
